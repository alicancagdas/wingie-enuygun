# GKE Cluster Terraform KonfigÃ¼rasyonu

Bu klasÃ¶r, Google Kubernetes Engine (GKE) cluster'Ä± ve ilgili altyapÄ± bileÅŸenlerini oluÅŸturmak iÃ§in Terraform konfigÃ¼rasyonlarÄ±nÄ± iÃ§erir.

## ğŸ—ï¸ AltyapÄ± Mimarisi

```mermaid
graph TB
    subgraph "GCP Project"
        subgraph "VPC Network"
            subgraph "Subnet (10.0.0.0/16)"
                subgraph "GKE Cluster (Single Zone)"
                    MP[Main Pool<br>1 Node<br>n2d-highcpu-2] --> SP[System Pods<br>kube-system]
                    AP[Application Pool<br>1-3 Nodes<br>n2d-highcpu-2] --> KP[Kubernetes Pods<br>User Applications]
                end
            end
            subgraph "IP Ranges"
                PI[Pod IPs<br>10.1.0.0/16]
                SI[Service IPs<br>10.2.0.0/16]
            end
        end
    end

style MP fill:#f9f,stroke:#333
style AP fill:#bbf,stroke:#333
style SP fill:#bfb,stroke:#333
style KP fill:#fbf,stroke:#333
style PI fill:#ddd,stroke:#333
style SI fill:#ddd,stroke:#333
```

## ğŸ“Š Resource DaÄŸÄ±lÄ±mÄ±

```mermaid
pie
    title Node Pool Kaynak DaÄŸÄ±lÄ±mÄ±
    "Main Pool (System)" : 1
    "Application Pool (Min)" : 1
    "Application Pool (Max)" : 2
```

## ğŸ”„ Autoscaling AkÄ±ÅŸÄ±

```mermaid
sequenceDiagram
    participant M as Metrics Server
    participant HPA as HPA/KEDA
    participant AP as Application Pool
    participant N as Nodes

    M->>HPA: CPU Metrics (%25 hedef)
    HPA->>AP: Scale Pods (1-3)
    AP->>N: Resource Request
    Note over N: Scale Out: ~2dk<br>Scale In: ~22dk
```

## ğŸ“ Terraform Kaynak YapÄ±sÄ±

```mermaid
graph LR
    subgraph "Network"
        VPC[VPC Network] --> SUB[Subnet]
        SUB --> CIDR1[Pod CIDR]
        SUB --> CIDR2[Service CIDR]
    end
    
    subgraph "GKE"
        CLUSTER[GKE Cluster] --> MP[Main Pool]
        CLUSTER --> APP[Application Pool]
        APP --> AS[Autoscaling<br>1-3 nodes]
    end

style VPC fill:#f9f
style CLUSTER fill:#bbf
style AS fill:#bfb
```

## âš™ï¸ Resource Ã–zellikleri

### Node Ã–zellikleri
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ n2d-highcpu-2                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”œâ”€ vCPU: 2                       â”‚
â”‚ â”œâ”€ Memory: 8GB                   â”‚
â”‚ â”œâ”€ Disk: 100GB                   â”‚
â”‚ â””â”€ OS: Container-Optimized OS    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Network YapÄ±landÄ±rmasÄ±
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Network Segmentation                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”œâ”€ VPC: Custom Network             â”‚
â”‚ â”œâ”€ Subnet: 10.0.0.0/16 (Primary)   â”‚
â”‚ â”œâ”€ Pods: 10.1.0.0/16 (Secondary)   â”‚
â”‚ â””â”€ Services: 10.2.0.0/16 (Secondary)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ” GÃ¼venlik YapÄ±landÄ±rmasÄ±

```mermaid
graph TD
    subgraph "Security Features"
        WI[Workload Identity] --> SA[Service Accounts]
        NP[Network Policy] --> FW[Firewall Rules]
        subgraph "Node Security"
            AR[Auto Repair]
            AU[Auto Upgrade]
            COS[Container-Optimized OS]
        end
    end
```

## ğŸ“ˆ Performans Metrikleri

### CPU KullanÄ±mÄ± ve Scaling
```
     CPU Utilization
100% â”¤
     â”‚     Scaling Trigger
 75% â”¤        â•­â”€â”€â•®
     â”‚        â”‚  â”‚
 50% â”¤     â•­â”€â”€â•¯  â•°â”€â”€â•®
     â”‚     â”‚        â”‚
 25% â”¤ â”€â”€â”€â”€â•¯        â•°â”€â”€â”€â”€
     â”‚
  0% â”¤
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        Time â†’
```

### Node Scaling Timeline
```
Nodes
  4 â”¤           â•­â”€â”€â”€â”€â”€â•®
    â”‚           â”‚     â”‚
  3 â”¤           â”‚     â”‚
    â”‚           â”‚     â”‚
  2 â”¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯     â•°â”€â”€â”€â”€â”€â”€
    â”‚
  1 â”¤
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      Time â†’
      â† 2min â†’ â† 22min â†’
```

## ğŸ¯ Resource Limitleri

### Application Pool
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Autoscaling Limits â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Min Nodes: 1       â”‚
â”‚ Max Nodes: 3       â”‚
â”‚ Min Pods: 1        â”‚
â”‚ Max Pods: 3        â”‚
â”‚ CPU Target: 25%    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pod Resources
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Container Limits     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CPU Request: 100m    â”‚
â”‚ CPU Limit: 200m      â”‚
â”‚ Memory Request: 128Miâ”‚
â”‚ Memory Limit: 256Mi  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## â° BakÄ±m Penceresi

```mermaid
gantt
    title BakÄ±m ZamanlamasÄ±
    dateFormat  YYYY-MM-DD
    section BakÄ±m
    Hafta Ä°Ã§i   :done,    des1, 2024-01-01, 2024-01-05
    BakÄ±m ZamanÄ±:active,  des2, 2024-01-06, 2024-01-07
    Hafta Ä°Ã§i   :done,    des3, 2024-01-08, 2024-01-12
```

## ğŸ”§ Kurulum AkÄ±ÅŸÄ±

```mermaid
graph LR
    A[initilate.sh] -->|1| B[Provider Setup]
    B -->|2| C[Backend Config]
    C -->|3| D[terraform init]
    D -->|4| E[terraform plan]
    E -->|5| F[terraform apply]
    F -->|6| G[GKE Cluster Ready]

style A fill:#f9f
style G fill:#bfb
```

## ğŸ“ Temel KonfigÃ¼rasyon DosyalarÄ±

### main.tf
Ana konfigÃ¼rasyon dosyasÄ±, aÅŸaÄŸÄ±daki kaynaklarÄ± oluÅŸturur:

1. **VPC Network**
   ```hcl
   resource "google_compute_network" "vpc"
   ```
   - Ã–zel VPC aÄŸÄ±
   - Auto-create subnetworks devre dÄ±ÅŸÄ±
   - Cluster adÄ±na gÃ¶re otomatik isimlendirme

2. **Subnet**
   ```hcl
   resource "google_compute_subnetwork" "subnet"
   ```
   - IP aralÄ±ÄŸÄ±: `10.0.0.0/16`
   - Pod IP aralÄ±ÄŸÄ±: `10.1.0.0/16`
   - Servis IP aralÄ±ÄŸÄ±: `10.2.0.0/16`

3. **GKE Cluster**
   ```hcl
   resource "google_container_cluster" "gke_cluster"
   ```
   - Tek zone'da Ã§alÄ±ÅŸÄ±r
   - Logging ve monitoring devre dÄ±ÅŸÄ±
   - Workload identity aktif
   - Network policy aktif
   - BakÄ±m penceresi: Hafta sonlarÄ±

4. **Node Pools**
   
   a. **Main Pool** (Sistem node'larÄ±)
   ```hcl
   resource "google_container_node_pool" "main_pool"
   ```
   - Machine type: `n2d-highcpu-2`
   - Sabit 1 node
   - Autoscaling yok
   
   b. **Application Pool** (Uygulama node'larÄ±)
   ```hcl
   resource "google_container_node_pool" "application_pool"
   ```
   - Machine type: `n2d-highcpu-2`
   - Min: 1 node, Max: 3 node
   - Autoscaling aktif

### variables.tf
DeÄŸiÅŸken tanÄ±mlamalarÄ± ve provider gereksinimleri:

```hcl
terraform {
  required_version = ">= 1.7.0"
  required_providers {
    google = "~> 6.36"
  }
}
```

**Temel DeÄŸiÅŸkenler:**
- `project_id`: GCP Proje ID'si
- `region`: VPC/Subnet bÃ¶lgesi (default: europe-west1)
- `zone`: GKE cluster zone'u (default: europe-west1-b)
- `cluster_name`: Cluster adÄ± (default: gke-cluster-demo)

### outputs.tf
OluÅŸturulan kaynaklarÄ±n Ã§Ä±ktÄ±larÄ±:

```hcl
output "network_name" { ... }    # VPC adÄ±
output "subnet_name" { ... }     # Subnet adÄ±
output "cluster_name" { ... }    # GKE cluster adÄ±
output "main_node_pool" { ... }  # Main pool adÄ±
output "app_node_pool" { ... }   # Application pool adÄ±
```

### terraform.tfvars
DeÄŸiÅŸken deÄŸerlerinin tanÄ±mlandÄ±ÄŸÄ± dosya:
```hcl
project_id = "fiery-iridium-460518-m8"
# Opsiyonel override'lar:
# zone = "europe-west1-c"
# region = "europe-west1"
# cluster_name = "my-cluster"
```

## ğŸš€ Autoscaling YapÄ±landÄ±rmasÄ±

### Node-level Autoscaling
Application Pool'da aktif:
- Minimum: 1 node
- Maximum: 3 node
- Trigger: Pod resource talepleri
- Scale-out sÃ¼resi: ~2 dakika
- Scale-in sÃ¼resi: ~22 dakika

### Pod-level Autoscaling
KEDA veya HPA ile yÃ¶netilir:
- CPU bazlÄ± Ã¶lÃ§eklendirme
- Minimum: 1 pod
- Maximum: 3 pod
- CPU hedefi: %25 kullanÄ±m

## ğŸ”§ Kurulum ve KullanÄ±m

1. **Terraform BaÅŸlatma**
   ```bash
   ./initilate.sh
   ```
   - Provider plugin'lerini indirir
   - Backend'i yapÄ±landÄ±rÄ±r
   - Workspace'i hazÄ±rlar

2. **Plan OluÅŸturma**
   ```bash
   terraform plan -out tfplan
   ```

3. **AltyapÄ± Kurulumu**
   ```bash
   terraform apply tfplan
   ```

## ğŸ—ï¸ AltyapÄ± Mimarisi

```
                           GKE Cluster (Single Zone)
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                               â”‚
              Main Pool (1)                 Application Pool (1-3)
                    â”‚                               â”‚
             System Pods                     Application Pods
```

## âš™ï¸ Resource Ã–zellikleri

### Node Ã–zellikleri
- **CPU**: 2 vCPU (n2d-highcpu-2)
- **Memory**: 8GB
- **Disk**: 100GB
- **OS**: Container-Optimized OS

### Network Ã–zellikleri
- **VPC Mode**: Custom
- **Pod CIDR**: /16 (65,536 IP)
- **Service CIDR**: /16 (65,536 IP)
- **Subnet CIDR**: /16 (65,536 IP)

## ğŸ“ Ã–nemli Notlar

1. **Maliyet Optimizasyonu**
   - Logging/monitoring devre dÄ±ÅŸÄ±
   - Minimum node sayÄ±sÄ± 2 (1 system + 1 application)
   - Spot instance kullanÄ±lmÄ±yor

2. **GÃ¼venlik**
   - Network policy aktif
   - Workload identity aktif
   - Private cluster deÄŸil

3. **BakÄ±m**
   - Otomatik bakÄ±m: Hafta sonlarÄ±
   - Node auto-repair: Aktif
   - Node auto-upgrade: Aktif 